/**
 * LLM Document Processor
 * Fallback service for processing documents to markdown using LLM when Docling is unavailable
 */

const dotenv = require("dotenv");
dotenv.config();

const { processDocument } = require("../utils/documentProcessor");
const fs = require("fs");
const path = require("path");

class LLMDocumentProcessor {
  constructor() {
    // Check if API key is available
    if (!process.env.GEMINI_API_KEY) {
      console.warn(
        "Warning: GEMINI_API_KEY not found in environment variables"
      );
      this.isConfigured = false;
      return;
    }

    // Set up direct API configuration
    this.apiKey = process.env.GEMINI_API_KEY;
    this.modelName = "gemini-2.5-flash-preview-05-20";
    this.isConfigured = true;

    // Set up processed markdown storage directory
    this.processedMarkdownDir = path.join(__dirname, "../processed-markdown");
    this.ensureProcessedMarkdownDir();

    console.log("‚úÖ LLM Document Processor configured with direct API");
  }

  /**
   * Ensure the processed markdown directory exists
   */
  ensureProcessedMarkdownDir() {
    try {
      if (!fs.existsSync(this.processedMarkdownDir)) {
        fs.mkdirSync(this.processedMarkdownDir, { recursive: true });
        console.log(
          `üìÅ Created processed markdown directory: ${this.processedMarkdownDir}`
        );
      }
    } catch (error) {
      console.warn(
        `Warning: Could not create processed markdown directory: ${error.message}`
      );
    }
  }

  /**
   * Save processed markdown to file
   * @param {string} markdownContent - The processed markdown content
   * @param {string} fileName - Original filename
   * @param {string} chatbotId - Chatbot ID for organization
   * @param {Object} metadata - Processing metadata
   * @returns {string} Path to saved markdown file
   */
  saveProcessedMarkdown(markdownContent, fileName, chatbotId, metadata) {
    try {
      // Create filename with timestamp and chatbot ID
      const timestamp = new Date().toISOString().split("T")[0]; // YYYY-MM-DD
      const timeString = new Date()
        .toISOString()
        .split("T")[1]
        .split(".")[0]
        .split(":")
        .join("-"); // HH-MM-SS
      const baseName = path.parse(fileName).name;
      const markdownFileName = `${timestamp}_${timeString}_${chatbotId}_${baseName}.md`;
      const markdownFilePath = path.join(
        this.processedMarkdownDir,
        markdownFileName
      );

      // Create content with metadata header
      const fileContent = `---
# Processed Markdown File
# Generated by LLM Document Processor
# Original File: ${fileName}
# Chatbot ID: ${chatbotId}
# Processing Method: ${metadata.processing_method}
# Processed At: ${metadata.processed_at}
# LLM Model: ${metadata.llm_model || "N/A"}
# Original Size: ${metadata.raw_text_length} chars
# Markdown Size: ${metadata.markdown_length} chars
---

${markdownContent}`;

      // Save the file
      fs.writeFileSync(markdownFilePath, fileContent, "utf8");
      console.log(`üíæ Saved processed markdown: ${markdownFileName}`);

      return markdownFilePath;
    } catch (error) {
      console.warn(
        `Warning: Could not save processed markdown: ${error.message}`
      );
      return null;
    }
  }

  /**
   * Process document to markdown using LLM fallback
   * @param {string} filePath - Path to the document file
   * @param {string} fileType - File extension (e.g., 'pdf', 'docx')
   * @param {string} fileName - Original filename
   * @param {string} title - Document title
   * @param {Object} options - Additional options (e.g., error context)
   * @returns {Promise<Object>} Processing result with markdown content
   */
  async processDocumentToMarkdown(
    filePath,
    fileType,
    fileName,
    title,
    options = {}
  ) {
    try {
      console.log(`ü§ñ Processing document with LLM: ${fileName} (${fileType})`);

      // Check if LLM is configured
      if (!this.isConfigured) {
        throw new Error(
          "LLM is not properly configured. Check GEMINI_API_KEY in environment variables."
        );
      }

      // Step 1: Extract raw text using legacy document processor
      let extractedText = "";

      try {
        extractedText = await this.extractRawText(filePath, fileType);
        console.log(
          `üìÑ Extracted ${extractedText.length} characters of raw text`
        );
      } catch (extractionError) {
        console.log(`‚ö†Ô∏è Text extraction failed: ${extractionError.message}`);

        // If we can't extract text, we can't process with LLM
        throw new Error(
          `Cannot process document: Text extraction failed - ${extractionError.message}`
        );
      }

      // Step 2: Process with LLM to convert to structured markdown
      const markdownContent = await this.convertTextToMarkdown(
        extractedText,
        title,
        fileName,
        fileType,
        options
      );

      // Step 3: Create metadata object
      const metadata = {
        processing_method: "llm_fallback",
        original_filename: fileName,
        file_type: fileType,
        file_size: fs.statSync(filePath).size,
        raw_text_length: extractedText.length,
        markdown_length: markdownContent.length,
        export_type: "markdown",
        docling_available: false,
        llm_model: "gemini-2.5-flash-preview-05-20",
        processed_at: new Date().toISOString(),
        ...(options.error && { docling_error: options.error }),
      };

      // Step 4: Save processed markdown to file (if chatbotId is available)
      let savedMarkdownPath = null;
      if (options.chatbotId) {
        savedMarkdownPath = this.saveProcessedMarkdown(
          markdownContent,
          fileName,
          options.chatbotId,
          metadata
        );
      }

      // Step 5: Return structured result
      return {
        success: true,
        markdownContent: markdownContent,
        metadata: {
          ...metadata,
          ...(savedMarkdownPath && { saved_markdown_path: savedMarkdownPath }),
        },
      };
    } catch (error) {
      console.error(`‚ùå LLM document processing failed: ${error.message}`);
      throw error;
    }
  }

  /**
   * Extract raw text from document using legacy processors
   * @param {string} filePath - Path to the document file
   * @param {string} fileType - File extension
   * @returns {Promise<string>} Extracted text content
   */
  async extractRawText(filePath, fileType) {
    try {
      // Handle different file types
      switch (fileType.toLowerCase()) {
        case "txt":
          return fs.readFileSync(filePath, "utf8");

        case "pdf":
        case "docx":
        case "doc":
          // Use legacy document processor for these formats
          return await processDocument(filePath, fileType);

        default:
          throw new Error(`Unsupported file type: ${fileType}`);
      }
    } catch (error) {
      throw new Error(
        `Text extraction failed for ${fileType}: ${error.message}`
      );
    }
  }

  /**
   * Convert raw text to structured markdown using LLM
   * @param {string} rawText - Raw extracted text
   * @param {string} title - Document title
   * @param {string} fileName - Original filename
   * @param {string} fileType - File type
   * @param {Object} options - Additional options
   * @returns {Promise<string>} Structured markdown content
   */
  async convertTextToMarkdown(
    rawText,
    title,
    fileName,
    fileType,
    options = {}
  ) {
    try {
      // Truncate text if too long (keep within token limits)
      const maxTextLength = 15000; // Conservative limit for processing
      const processText =
        rawText.length > maxTextLength
          ? rawText.substring(0, maxTextLength) +
            "\n\n[Content truncated due to length...]"
          : rawText;

      const prompt = this.buildMarkdownConversionPrompt(
        processText,
        title,
        fileName,
        fileType,
        options
      );

      console.log(
        `üß† Converting to markdown with LLM (${processText.length} chars)...`
      );

      // Use direct Google Generative AI API
      const markdownContent = await this.callGeminiAPI(prompt);

      // Check if LLM returned content
      if (!markdownContent) {
        console.warn("Warning: LLM returned empty content");
        throw new Error("LLM returned empty content");
      }

      // Validate and clean the markdown
      const cleanedMarkdown = this.cleanAndValidateMarkdown(
        markdownContent,
        title
      );

      console.log(
        `‚úÖ LLM conversion complete (${cleanedMarkdown.length} chars)`
      );
      return cleanedMarkdown;
    } catch (error) {
      throw new Error(`LLM markdown conversion failed: ${error.message}`);
    }
  }

  /**
   * Call Google Generative AI API directly
   * @param {string} prompt - The prompt to send to the API
   * @returns {Promise<string>} The generated content
   */
  async callGeminiAPI(prompt) {
    try {
      const apiUrl = `https://generativelanguage.googleapis.com/v1beta/models/${this.modelName}:generateContent?key=${this.apiKey}`;

      const requestBody = {
        contents: [
          {
            parts: [
              {
                text: prompt,
              },
            ],
          },
        ],
        generationConfig: {
          temperature: 0.1,
          maxOutputTokens: 8000,
          topP: 0.8,
          topK: 10,
        },
      };

      const response = await fetch(apiUrl, {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
        },
        body: JSON.stringify(requestBody),
      });

      if (!response.ok) {
        const errorText = await response.text();
        throw new Error(
          `API request failed: ${response.status} - ${errorText}`
        );
      }

      const data = await response.json();

      if (
        !data.candidates ||
        !data.candidates[0] ||
        !data.candidates[0].content
      ) {
        throw new Error("Invalid API response structure");
      }

      const generatedText = data.candidates[0].content.parts[0].text;
      return generatedText;
    } catch (error) {
      console.error("Error calling Gemini API:", error);
      throw new Error(`Gemini API call failed: ${error.message}`);
    }
  }

  /**
   * Build prompt for LLM markdown conversion
   * @param {string} text - Raw text to convert
   * @param {string} title - Document title
   * @param {string} fileName - Original filename
   * @param {string} fileType - File type
   * @param {Object} options - Additional options
   * @returns {string} LLM prompt
   */
  buildMarkdownConversionPrompt(text, title, fileName, fileType, options = {}) {
    return `You are a document processing expert. Convert the following raw text extracted from a ${fileType.toUpperCase()} file into well-structured markdown format.

**Instructions:**
1. Create a clear document structure with appropriate headings (# ## ###)
2. Preserve the original content meaning and information
3. Format lists, tables, and other structures properly in markdown
4. Remove any extraction artifacts or formatting noise
5. Ensure the content flows logically
6. Use the provided title as the main heading
7. Add appropriate section breaks and organization

**Document Information:**
- Title: ${title}
- Filename: ${fileName}
- File Type: ${fileType}
${
  options.error
    ? `- Note: This is a fallback processing due to primary processor error`
    : ""
}

**Raw Text to Convert:**
\`\`\`
${text}
\`\`\`

**Requirements for Output:**
- Start with # ${title} as the main heading
- Use proper markdown syntax throughout
- Maintain all important information from the original
- Create logical sections and subsections
- Format any lists, tables, or structured data appropriately
- End with a processing note

Please convert this text to clean, well-structured markdown:`;
  }

  /**
   * Clean and validate the generated markdown
   * @param {string} markdown - Raw markdown from LLM
   * @param {string} title - Document title
   * @returns {string} Cleaned markdown
   */
  cleanAndValidateMarkdown(markdown, title) {
    try {
      // Handle undefined or null markdown
      if (!markdown || typeof markdown !== "string") {
        console.warn(
          "Warning: LLM returned invalid markdown content, creating fallback"
        );
        return this.createFallbackMarkdown(
          title,
          "LLM returned empty or invalid content"
        );
      }

      // Remove any code block wrappers if present
      let cleaned = markdown;
      if (cleaned.startsWith("```markdown")) {
        cleaned = cleaned.substring(11);
      }
      if (cleaned.startsWith("```")) {
        cleaned = cleaned.substring(3);
      }
      if (cleaned.endsWith("```")) {
        cleaned = cleaned.substring(0, cleaned.length - 3);
      }

      // Check if cleaned content is empty or too short
      if (!cleaned || cleaned.trim().length < 10) {
        console.warn(
          "Warning: LLM returned very short content, creating fallback"
        );
        return this.createFallbackMarkdown(
          title,
          "LLM returned insufficient content"
        );
      }

      // Ensure it starts with the title if not present
      if (!cleaned.trim().startsWith("#")) {
        cleaned = `# ${title}\n\n${cleaned}`;
      }

      // Add processing note at the end
      const processingNote = `\n\n---\n*Document processed using LLM fallback at ${new Date().toISOString()}*`;

      if (!cleaned.includes("processed using LLM fallback")) {
        cleaned += processingNote;
      }

      // Basic cleanup
      // Remove excessive line breaks
      while (cleaned.includes("\n\n\n")) {
        cleaned = cleaned.split("\n\n\n").join("\n\n");
      }

      // Trim whitespace
      cleaned = cleaned.trim();

      // Normalize line endings
      cleaned = cleaned.split("\r\n").join("\n");

      return cleaned;
    } catch (error) {
      console.warn(`Warning: Markdown cleaning failed: ${error.message}`);
      // Return fallback content if cleaning fails
      return this.createFallbackMarkdown(
        title,
        `Markdown processing error: ${error.message}`
      );
    }
  }

  /**
   * Create fallback markdown content when LLM processing fails
   * @param {string} title - Document title
   * @param {string} reason - Reason for fallback
   * @returns {string} Fallback markdown content
   */
  createFallbackMarkdown(title, reason) {
    return `# ${title}

## Processing Notice
This document could not be fully processed by the LLM fallback system.

**Reason**: ${reason}

## Next Steps
1. Check if the original document contains readable text
2. Try re-uploading the document
3. Verify the document is not corrupted or password-protected
4. Consider converting to a different format (PDF, TXT, DOCX)

## Manual Processing
If this issue persists, you may need to:
- Extract the text manually from the document
- Upload the content as a text entry instead
- Contact support for assistance

---
*Fallback content generated at ${new Date().toISOString()}*`;
  }

  /**
   * Check if LLM processing is available
   * @returns {Promise<boolean>} True if LLM is available
   */
  async isAvailable() {
    try {
      if (!this.isConfigured || !process.env.GEMINI_API_KEY) {
        return false;
      }

      // Quick test with minimal content
      const testResult = await this.callGeminiAPI(
        "Test: Convert 'Hello World' to markdown."
      );
      return testResult && testResult.length > 0;
    } catch (error) {
      console.warn(`LLM availability check failed: ${error.message}`);
      return false;
    }
  }

  /**
   * Get processing capabilities
   * @returns {Object} Supported file types and features
   */
  getCapabilities() {
    return {
      supportedFileTypes: ["txt", "pdf", "docx", "doc"],
      features: [
        "text_extraction",
        "markdown_conversion",
        "structure_detection",
        "content_organization",
        "format_cleanup",
      ],
      limitations: [
        "requires_text_extraction",
        "token_length_limits",
        "processing_time_longer",
        "quality_depends_on_source",
      ],
      fallbackFor: ["docling_service_unavailable", "docling_processing_errors"],
    };
  }
}

module.exports = LLMDocumentProcessor;
